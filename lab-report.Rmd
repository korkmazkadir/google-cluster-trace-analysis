---
title: "Google Cluster Trace Analysis with Apache Spark"
author: "Kadir Korkmaz"
date: "19/01/2019"
description: This is my template
output:
  pdf_document: 
    toc: yes
  html_document: default
---


\pagebreak

#Abstract


> In this worked, I have analyzed the Google Cluster trace using Apache Spark. I have used the Data Frame API of Spark. I have conducted proposed analyses. Google Cluster trace contains data for 29 days. In the cluster, there are 12,583 machines. In the cluster, there is only 10 different kind of machine when we consider the CPU and memory. More than 90% of the machines have 0.5 CPU capacity. Only 6.32 percent of the machines have the highest CPU and memory capacity. Machines have three different kinds of hardware platforms. During the trace period, 672.004 Job is scheduled. Every job has one or more task. The total number of tasks is 25.4 million. The average number of task per job is 37.83. Because of eviction and killing scheduled task count 47.3 million. There is no linear relation between the priority of a task and probability of eviction but it is clear that some priority levels have a higher evicted and killed task percentage. There is no relation between the priority of a task and resources of the machines which task is scheduled. Tasks are almost uniformly distributed on machines. There is tasks which consume significantly fewer resources than what they requested. In the cluster, there is a maintenance or machine failure for every 279 seconds. There are 36 machines which are underutilized. 

>The number of worker threads has a big effect on performance if there is enough input file to process for every worker thread. JVM memory size could affect performance in different ways. Caching has a big impact on performance. If it is possible to reuse an RDD, caching increasing performance linearly.



#Introduction
In this work, I analysed [Google cluster-usage traces](https://github.com/google/cluster-data/blob/master/ClusterData2011_2.md) using Apache Spark and I created the document using R Studio.

You can access the subject of this work from [this](https://tropars.github.io/downloads/lectures/LSDM/LSDM-lab-spark-google.pdf) link.
You can find the detailed documantatin of traces from [this](https://drive.google.com/file/d/0B5g07T_gRDg9Z0lsSTEtTWtpOW8/view)  link.

**This version of the code does not contains the Spark codes which are used to extract information from the traces also this is not the final version. There is no guarante related to correctness of the results.**

```{r setup, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(gridExtra)
library(grid)
library(data.table)
library(ggplot2)

formatNumber<-function(number){
  return(format(number, big.mark=","))
}

formatDataFrame<-function(df){
  return(format.data.frame(df, big.mark=","))
}

```

\pagebreak

#Trace Analysis of the Cluster
##1. Machine Distibution

In this section, I have showed the distribution of the machines according to CPU and memory capacity. Also, I have showed machine distribution according to paltform type but this information is not significant because machine distirbution according to platform type is same with according to CPU

### Machine Distibution According to CPU and Memory Capacity

I have used machine events table to calculate machine distribution. In this table we have CPU and memory information of the machines. I have ordered data in decending order according to time then I have grouped by according to machine id. I get the first cpu and memory information from every group. So this machine distribution shows the final state of the cluster. It is possible to get slightly different machine distirbution according to ordering because cluster is not something static. It is changing with time. Graph 1 shows the machibe distribution according CPU and memory. From this graph it is easy to observe that more than 90% of the machines have 0.5 CPU and very few machines have 0.25 CPU.   

&nbsp;
&nbsp;

```{r echo=FALSE}
library(ggplot2)
library(ggExtra)
library(knitr)

machineDistribution = 
  read.csv("./result/question-1/machine-dist-cpu-memory-capacity.csv", header = TRUE)


p=ggplot(machineDistribution, aes(x=cpu, y=memory, color=cpu, size=number_of_machines, label=number_of_machines)) +
      geom_point() + 
      geom_text(aes(label=number_of_machines),hjust=0, size=4, nudge_x= 0.025) +
      theme(legend.position="none") 

p +  
  scale_x_continuous(breaks=c(0.25,0.5,1)) + 
  ggtitle("Graph 1: Machine distribution according to CPU and Memory") +
  theme(plot.title = element_text(hjust = 0.5, size = 9))

#+ scale_y_continuous(breaks=unique(machineDistribution$memory))

```


\pagebreak

Table 1 show the same information with Graph 1 additionally it shows **capacity** metric and percentages of the machines in every capacity. I have created capacity metric. Capacity is average value of CPU and Memory capacity( Capacity = (CPU + Memory) / 2 ). This table shows the percentages of machines for every capacity. In next sections I will use this metric on plots.


```{r, echo=FALSE}
kable(machineDistribution, caption="Machine distribution according to capacity")
```




### Machine Distibution According to Platform

Machine distribution acording to platform is not significant because it is same with CPU distirbution. On the documentation it is stated that two machines with same platform id can have very different CPU and memory resources. Table 2 shows the machine distribution acccording to platform.


```{r echo=FALSE}

machineDistribution = 
  read.csv("./result/question-1/machine-dist-platform.csv", header = TRUE)

kable(machineDistribution, caption="Machine distribution according to platform id")

```

\pagebreak

##2. On average, how many tasks compose a job?s

I have used task events table to count the number of jobs and number of job per task. I have select distinct rows according to job_id and task_index then I grouped by according to job_id to count the number of task per job. Table 3 shows the total number of jobs, total number of tasks and average number of task per job.


```{r, echo=FALSE}
jobTaskCount = 
  read.csv("./result/question-2/job-task-count.csv", header = TRUE)

df <- data.frame(
  "number_of_jobs" = c(formatNumber( nrow(jobTaskCount) )), 
  "number_of_tasks" = c(formatNumber( sum(jobTaskCount$task_count) )),
  "avg_number_of_task_count" = c(formatNumber( mean(jobTaskCount$task_count) ))
  )

kable(df, caption="Job and task counts")

```

Table 4 shows the statistics about number of tasks per job. From this statistics we can conclude that big part of the jobs contains only 1 task. Average number of task per job is **37.8342**. The job with biggest number of task has **90050** task.


```{r,  echo=FALSE}
summary<-summary(jobTaskCount$task_count)

summaryDF <- data.frame(unclass(summary), check.names = FALSE, stringsAsFactors = FALSE)
colnames(summaryDF)<- c("statistics")

kable(summaryDF, caption="Summary statistics of the number of task per job")
```



##3. Job/Task that got killed or evicted

I have used job table and task event tables to count the number of evicted and killed jobs and tasks. Table 5 shows the figures related to job/task schedule, evicted and killed events.




```{r,  echo=FALSE}
# jobCounts = 
#   read.csv("./result/question-3/job-counts.csv", header = TRUE)
# 
# taskCounts = 
#   read.csv("./result/question-3/task-counts.csv", header = TRUE)


jobTaskCouns <- data.frame(
  "type" = c("Job","Task"),
  #"unique_count" = c(672075, 25424731), 
  "scheduled_count" = c(672075, 47351173),
  "evicted_count" = c(22,5864353),
  "killed_count" = c(272341,103496805)
  )


kable(formatDataFrame (jobTaskCouns) , caption = "Job/task evicted killed count")

```





\pagebreak

##4. Eviction Probability of Tasks with Respect to Priority

I have used task events table to calculate efiction percentages of tasks accordign to priority. I have count the number of events according to priority and than I have calculated percentages of events on every priority. Table 5 shows the percentages of events according to priority. In this table, percentages are calculated according to task counts. *task_count* column shows the number of **unique** task on every priority level.

**When we analyse the data on the table we can not see any linear relation between priority of task and eviction counts however some priority levels have higher eviction percentages. On the documentation it had been stated that there are some special priority ranges : "free", "production" and "monitoring".**

```{r, echo=FALSE}
priorityLifeCycleStats = 
  read.csv("./result/question-4/priority-task-life-cycle.csv", header = TRUE)

colnames(priorityLifeCycleStats)[colnames(priorityLifeCycleStats) == 'evicted_task_count'] <- 'evicted_count'
colnames(priorityLifeCycleStats)[colnames(priorityLifeCycleStats) == 'failed_task_count'] <- 'failed_count'
colnames(priorityLifeCycleStats)[colnames(priorityLifeCycleStats) == 'killed_task_count'] <- 'killed_count'
colnames(priorityLifeCycleStats)[colnames(priorityLifeCycleStats) == 'lost_task_count'] <- 'lost_count'
colnames(priorityLifeCycleStats)[colnames(priorityLifeCycleStats) == 'distinct_task_count'] <- 'task_count'

priorityLifeCycleStats$evicted_percent = round( priorityLifeCycleStats$evicted_count * 100 / priorityLifeCycleStats$task_count, 1 )
priorityLifeCycleStats$failed_percent =  round( priorityLifeCycleStats$failed_count * 100 / priorityLifeCycleStats$task_count, 1 )
priorityLifeCycleStats$killed_percent =  round( priorityLifeCycleStats$killed_count * 100 / priorityLifeCycleStats$task_count, 1 )
priorityLifeCycleStats$lost_percent =  round( priorityLifeCycleStats$lost_count * 100 / priorityLifeCycleStats$task_count, 1 )

#kable( formatDataFrame( priorityLifeCycleStats[,c("priority", "evicted_count", "failed_count", "killed_count", "lost_count", "task_count")]))
kable( formatDataFrame( priorityLifeCycleStats[,c("priority","killed_percent", "evicted_percent", "failed_percent", "lost_percent", "task_count")]),
       caption = "Task event percentages according to priority of tasks")


```

\pagebreak

##5. Relation between the Priority of a Task and Machine Resources(CPU, Memory)

Is there any relation between priority of tasks and machine resources which tasks are scheduled? I have used task events and machine events tables to answer this question. I have calculated distinct machine table from machine events and I have joined it with task events table than I have grouped by priority and *capacity* then I have count the number of  tasked scheduled.

Graph 2 show a heat map show the relation. In y axis we have the priorities of the tasks and on the x axis we have the capacity of the machines which are tasked scheduled. On the cell's we have the percentages of tasks which are schedulet on that knd of machines. Table 6 shows number of tasks according to priority of task. Also Table 1 shows the percentages of the machines according to capacity. **When we consider all the information on the heat map and tables we can conclude that tasks are uniformly distributed on the machines. There is no relation between priority of task and capacity of the machines.**

For example, In the cluster, 53.5% of the machines have capacity of 0.49975(from Table 1). When we look at the values of the cell's on the heat map with capacit 0.49975 we can see that approximatly 55% of the tasks on all categories are scheduled on this kind of machines. We can do same observation for other capacities also. 

&nbsp;
&nbsp;
&nbsp;

```{r echo=FALSE, fig.height = 4}
library(ggplot2)
library(ggExtra)
library(knitr)


priorityCapacityDataset = 
  read.csv("./result/question-5/priority-machine-capacity-relation.csv", header = TRUE)


# p=ggplot(priorityCapacityDataset, aes(x=capacity, y=priority, size=task_count, label=task_count)) +
#       geom_point() + 
#       #geom_text(aes(label=task_count),hjust=0, size=4, nudge_x= 0.025) +
#       theme(legend.position="none", axis.text.x = element_text(angle = 50, hjust = 1)) 
# 
# p +  scale_x_continuous(breaks=priorityCapacityDataset$capacity) + 
#   scale_y_continuous(breaks=unique(priorityCapacityDataset$priority))

taskCountByPriority <- aggregate(priorityCapacityDataset$task_count, by=list(Category=priorityCapacityDataset$priority), FUN=sum)

colnames(taskCountByPriority) <- c("priority","total_task_count")

#kable( formatDataFrame( taskCountByPriority ), caption="Task count by priority")

machineDistribution = 
  read.csv("./result/question-1/machine-dist-cpu-memory-capacity.csv", header = TRUE)
#kable(machineDistribution[,c("capacity","cpu", "memory", "machine_percentage")], caption="Machine distribution by capacity")


priorityCapacityDataset <- merge(priorityCapacityDataset, taskCountByPriority, by = "priority")

priorityCapacityDataset$task_percentage = round( priorityCapacityDataset$task_count * 100 /  priorityCapacityDataset$total_task_count, 1 ) 

#Makes disceret axis
priorityCapacityDataset$capacity <- as.character(priorityCapacityDataset$capacity)
priorityCapacityDataset$priority <- as.character(priorityCapacityDataset$priority)

#Order Values numerically
priorityCapacityDataset$priority <- factor(priorityCapacityDataset$priority, levels = c("0","1","2","3","4","5","6","7","8","9","10","11"))

ggplot(data = priorityCapacityDataset, aes(x = capacity, y = priority)) +
  geom_tile(aes(fill = task_percentage), colour = "white") +
  geom_text(aes(label=task_percentage),size=2,color="linen") +
  theme(legend.position="none",axis.text.x = element_text(angle = 50, hjust = 1), panel.grid.major  = element_blank(),  panel.grid.minor  = element_blank()) +
  ggtitle("Graph 2: Machine capacity and priority heat map") +
  theme(plot.title = element_text(hjust = 0.5, size = 9))
```

\pagebreak

Table 6 shows the task distribution according to piority. In this table, task counts are not unique task counts because some tasks are rescheduled because of eviction or failure.

```{r, echo=FALSE}
kable( formatDataFrame( taskCountByPriority ), caption = 'Task counts according to priority of the task')
#kable(machineDistribution, caption="Machine distribution according to capacity")
```


##6. Tasks consumes significantly less resource than what they requested

I have used special method to decide whether or not a task is using significantly less resource from what it requested. To decide this, I have looked at the tasks's maximum CPU and memory resource consumption. If both of the maximum values never reaches what it requested than I assumed that this task is using significantly less resource from what it requested. I have used **task events** and **task usage** tables to find the specific tasks. I have joined this two table then I have group by according to job_id and task_index and I have looked at the max velues of maximum cpu usage and maximum memory usage. Then I have decided whether or not a task using significantly less resource.

I have decided to create a histogram to have better understanding the resoruce consumption behavior of tasks which are using significantly less recoruces from what they requested. I have created two histogram. On the **X** axis I showed the resource consumption percentage according to what it requested on the **Y** I have showed the number of tasks. Graph 3 shows the memory consumption behavior and Graph 4 shows the CPU consumpsion behavior of the tasks which are usign significantly less resoruces from what they requested.

```{r results='asis', echo=FALSE, fig.height = 2.5}

bins <- c(0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100)

memoryConsumption <- c(9403495, 162321, 212388, 488168, 528610, 594301, 829299, 598128, 485174, 514702, 602390, 509211, 521277, 507115, 512200, 559509, 624598, 661028, 796245, 1051587)

cpuComnsumprion <- c(11828731, 3196895, 1439782, 1691237, 685753, 321222, 225548, 168018, 123777, 106177, 69096, 142918, 36677, 17153, 17901, 19113, 15198, 15081, 22789, 18680)

consumptionDF <- data.frame(
  "percentage"= c(2.5, 7.5, 12.5, 17.5, 22.5, 27.5, 32.5, 37.5, 42.5, 47.5, 52.5, 57.5, 62.5, 67.5, 72.5, 77.5, 82.5, 87.5, 92.5, 97.5),
  "memory_consumption" = c(9403495, 162321, 212388, 488168, 528610, 594301, 829299, 598128, 485174, 514702, 602390, 509211, 521277, 507115, 512200, 559509, 624598, 661028, 796245, 1051587),
  "cpu_consumption" = c(11828731, 3196895, 1439782, 1691237, 685753, 321222, 225548, 168018, 123777, 106177, 69096, 142918, 36677, 17153, 17901, 19113, 15198, 15081, 22789, 18680)
)

p1 <- ggplot(consumptionDF, aes(x=percentage, y=memory_consumption)) +
  geom_bar(stat="identity") + scale_x_continuous(breaks=c(0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100)) +
  theme(legend.position="none", axis.text.x = element_text(angle = 50, hjust = 1, size=6)) + scale_y_log10() + labs(y = "Number of Tasks", x="resource consumption percentage") +
  ggtitle("Graph 3: Tasks consumes significantly less memory") +
  theme(plot.title = element_text(hjust = 0.5, size = 8))

p2 <- ggplot(consumptionDF, aes(x=percentage, y=cpu_consumption)) +
  geom_bar(stat="identity") + scale_x_continuous(breaks=c(0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100)) +
  theme(legend.position="none", axis.text.x = element_text(angle = 50, hjust = 1, size=6)) + scale_y_log10() + labs(y = "Number of Tasks", x="resource consumption percentage") +
  ggtitle("Graph 4: Tasks consumes significantly less CPU") +
  theme(plot.title = element_text(hjust = 0.5, size = 8))

grid.arrange(p1, p2, nrow = 1)

```



##7. Machine availability

I have conducted some analysis to have idea related to machine availibility. 
I think this analysis can be useful to answer the next question because


*1- What is the average time between two consecutive machine remove event.*

>Average time between two consecutive remove event is *279.7* Seconds.
>
>According to the documentation, machine remove events can occure because of failures or maintenance
>so we can conclude that every *279.7* second there is a failure or maintanence in the cluster.

*2- Are there any machine which is removed from cluster and did not added to back.*

> *97* of the machines are removed from cluster and they are not added back.
> This number could be helpful to make following analysis.

##8. Machines which resources are largely underused 

In the cluster, there are 36 machines wich highly underused. I have query all the machine usage data to find the machines which are under used.

```{r, echo=FALSE}

machineUtilization = 
  read.csv("./result/question-6/scheduled-task-counts-on-machines.csv", header = TRUE)


p<-ggplot(machineUtilization, aes(x=scheduled_task_count)) + 
  geom_histogram(color="black", fill="white", binwidth=50)

p <- p + ggtitle("Graph 5: Scheduled task count per machine histogram") + theme(plot.title = element_text(hjust = 0.5, size = 8))

p


```


\pagebreak

```{r, echo=FALSE}
summaryDF <- data.frame(unclass(summary(machineUtilization$scheduled_task_count)), check.names = FALSE, stringsAsFactors = FALSE)
colnames(summaryDF)<- c("statistics")
kable( summaryDF, caption = "Machine scheduled task count statistics")
```





#Performance Analyses of Apache Spark

##1. Efect of Number of Worker Thread

If there is enough input file to process than in this case number of worker threads effecting performance. If there is only one input file to process than number of worker threads has small performance effect.This Effect is not linear. For example to count all the distinct jobs in job tables with 8 worker thread takes 43 seconds. Same process takes 120 seconds with only one thread. For only one input file with single worker thread same job takes 15 seconds and with 8 worker thread it takes 14 seconds.


##2. Efect of JVM Memory Size

Normally in the spark cluster there is two kind of process driver and executor.There is two different parameter to control the JVM memory size. These parameters are "spark.driver.memory" and "spark.executor.memory". Normall this two parameter have different effect but in our case "spark.executor.memory" parameter is not useful because we are running Spark in local mode which means that driver process spawning executor threads to do the processing so there is only one JVM which is dirver JVM. Because of this reason I can only assess the effect of "spark.driver.memory" parameter.

When we increase the memory size of the driver process, we can easily observe via using system tools(system monitor, top, htop) driver JVM is instantiated with specified memory and it is using the reserved memory. Also for caching spark is using  memory on default so if there is enough memory caching effect could be big incase of all the cached data small enoug to fit in memory.

During my study, I have java.lang.OutOfMemoryError incase of collecting huge amount of data at driver process. Arranging JVM memory size could solve this problem.

##3. Effect of Lazy Evaluation



##4. Effect of Cache

Cache has a big impect to performance. In normal conditions spark is not caching RDD's. Assume that you are making a calculation and you are counting the number of elements in the RDD later you tried to save the RDD to a csv file. Without caching Spark will calculate same RDD two times, once for counting once for writing to csv file. With cachign it will only calculate once and it will use the result of the same calculation for counting and writing to csv file.

At teh beginning of this study I was showing result of the RDD's and later I was tring to do further processing without caching. In this case, it was taking two times more time to finih the job. Later, I realized the effect of caching and I tried to used caching when it is necessary.

#Conclution


