---
title: "Google Cluster Trace Analysis with Apache Spark"
author: "Kadir Korkmaz"
date: "19/01/2019"
description: This is my template
output:
  pdf_document: 
    toc: yes
  html_document: default
---


\pagebreak

#Abstract


> In this work, I have analyzed the Google Cluster trace using Apache Spark. I have used the Data Frame API of Spark. I have conducted proposed analyses. Google Cluster trace contains data for 29 days. In the cluster, there are 12,583 machines. In the cluster, there is only 10 different kind of machine when we consider the CPU and memory. More than 90% of the machines have 0.5 CPU capacity. Only 6.32 percent of the machines have the highest CPU and memory capacity. Machines have three different kinds of hardware platforms. During the trace period, 672.004 Job is scheduled. Every job has one or more task. The total number of tasks is 25.4 million. The average number of task per job is 37.83. Because of eviction and killing scheduled task count 47.3 million. There is no linear relation between the priority of a task and probability of eviction but it is clear that some priority levels have a higher evicted and killed task percentage. There is no relation between the priority of a task and resources of the machines which task is scheduled. Tasks are almost uniformly distributed on machines. There are tasks which consume significantly fewer resources than what they requested. In the cluster, there is a maintenance or machine failure for every 279 seconds. There is a machine which never a task scheduled. There are 45 machines which have less than 10 tasks scheduled. There are 52 machines which have not finished any task.

>The number of worker threads has a big effect on performance if there is enough input file to process for every worker thread. JVM memory size affecting performance in different ways. Lazy evaluation mechanism of Spark creates an opportunity to Spark optimize user requests to return the results in a shortest possible time. Caching has a big impact on performance. If it is possible to reuse an RDD, caching increasing performance linearly.



#Introduction

In this work, I have analyzed [Google cluster-usage traces](https://github.com/google/cluster-data/blob/master/ClusterData2011_2.md) using Apache Spark and I have created the document using R Studio.

You can access the subject of this work from [this](https://tropars.github.io/downloads/lectures/LSDM/LSDM-lab-spark-google.pdf) link.
You can find the detailed documentation of traces from [this](https://drive.google.com/file/d/0B5g07T_gRDg9Z0lsSTEtTWtpOW8/view)  link.

You can access the details of this work and materials(source code, intermadiate results, CSV files) from [this](https://github.com/korkmazkadir/google-cluster-trace-analysis) link.


```{r setup, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(gridExtra)
library(grid)
library(data.table)
library(ggplot2)

formatNumber<-function(number){
  return(format(number, big.mark=","))
}

formatDataFrame<-function(df){
  return(format.data.frame(df, big.mark=","))
}

```

\pagebreak

#Trace Analysis of the Cluster
##1. Machine Distribution

In this section, I have shown the distribution of the machines according to CPU and memory capacity. Also, I have shown machine distribution according to platform type but this information is not significant because machine distribution according to platform type is same with according to CPU distribution.

### Machine Distribution According to CPU and Memory Capacity

I have used machine events table to calculate machine distribution. In this table, we have CPU and memory information of the machines. I have ordered data in descending order according to time then I have grouped by according to machine id. I get the first CPU and memory information from every group. So this machine distribution shows the final state of the cluster. It is possible to get slightly different machine distribution according to order because the cluster is not something static. It is changing with time. Graph 1 shows the machine distribution according to CPU and memory. From this graph, it is easy to observe that more than 90% of the machines have 0.5 CPU and very few machines have 0.25 CPU.   

&nbsp;
&nbsp;

```{r echo=FALSE}
library(ggplot2)
library(ggExtra)
library(knitr)

machineDistribution = 
  read.csv("./result/question-1/machine-dist-cpu-memory-capacity.csv", header = TRUE)


p=ggplot(machineDistribution, aes(x=cpu, y=memory, color=cpu, size=number_of_machines, label=number_of_machines)) +
      geom_point() + 
      geom_text(aes(label=number_of_machines),hjust=0, size=4, nudge_x= 0.025) +
      theme(legend.position="none") 

p +  
  scale_x_continuous(breaks=c(0.25,0.5,1)) + 
  ggtitle("Graph 1: Machine distribution according to CPU and Memory") +
  theme(plot.title = element_text(hjust = 0.5, size = 9))

#+ scale_y_continuous(breaks=unique(machineDistribution$memory))

```


\pagebreak

Table 1 show the same information with Graph 1 additionally it shows **capacity** metric and percentages of the machines in every capacity. I have created the capacity metric. Capacity is average value of CPU and Memory capacity( Capacity = (CPU + Memory) / 2 ). This table shows the percentages of machines for every capacity. In the next sections, I will use this metric on plots.


```{r, echo=FALSE}
kable(machineDistribution, caption="Machine distribution according to capacity")
```




### Machine Distibution According to Platform

Machine distribution according to the platform is not significant because it is the same with CPU distribution. On the documentation, it is stated that two machines with the same platform id can have very different CPU and memory resources. Table 2 shows the machine distribution according to the platform.


```{r echo=FALSE}

machineDistribution = 
  read.csv("./result/question-1/machine-dist-platform.csv", header = TRUE)

kable(machineDistribution, caption="Machine distribution according to platform id")

```

\pagebreak

##2. On average, how many tasks compose a jobs?

I have used task events table to count the number of jobs and number of task per job. I have select distinct rows according to job_id and task_index then I grouped by according to job_id to count the number of task per job. Table 3 shows the total number of jobs, the total number of tasks and the average number of task per job.

```{r, echo=FALSE}
jobTaskCount = 
  read.csv("./result/question-2/job-task-count.csv", header = TRUE)

df <- data.frame(
  "number_of_jobs" = c(formatNumber( nrow(jobTaskCount) )), 
  "number_of_tasks" = c(formatNumber( sum(jobTaskCount$task_count) )),
  "avg_number_of_task_count" = c(formatNumber( mean(jobTaskCount$task_count) ))
  )

kable(df, caption="Job and task counts")

```

Table 4 shows the statistics about the number of tasks per job. From these statistics we can conclude that a big part of the jobs contains only 1 task. An average number of task per job is **37.8342**. The job with the biggest number of the task has **90050** task.

```{r,  echo=FALSE}
summary<-summary(jobTaskCount$task_count)

summaryDF <- data.frame(unclass(summary), check.names = FALSE, stringsAsFactors = FALSE)
colnames(summaryDF)<- c("statistics")

kable(summaryDF, caption="Summary statistics of the number of task per job")
```



##3. Job/Task that got killed or evicted

I have used job and task event tables to count the number of evicted and killed jobs and tasks. Table 5 shows the figures related to the job/task schedule, evicted and killed events.



```{r,  echo=FALSE}
# jobCounts = 
#   read.csv("./result/question-3/job-counts.csv", header = TRUE)
# 
# taskCounts = 
#   read.csv("./result/question-3/task-counts.csv", header = TRUE)


jobTaskCouns <- data.frame(
  "type" = c("Job","Task"),
  #"unique_count" = c(672075, 25424731), 
  "scheduled_count" = c(672075, 47351173),
  "evicted_count" = c(22,5864353),
  "killed_count" = c(272341,103496805)
  )


kable(formatDataFrame (jobTaskCouns) , caption = "Job/task evicted killed count")

```





\pagebreak

##4. Eviction Probability of Tasks with Respect to Priority

I have used task events table to calculate eviction percentages of tasks according to priority. I have counted the number of events according to priority and then I have calculated percentages of events on every priority. Table 6 shows the percentages of events according to priority. In this table, percentages are calculated according to task counts. *task_count* column shows the number of **unique** task on every priority level.

**When we analyze the data on the table we can not see any linear relationship between the priority of task and eviction counts, however, some priority levels have higher eviction percentages. On the documentation it had been stated that there are some special priority ranges as "free", "production" and "monitoring".**

```{r, echo=FALSE}
priorityLifeCycleStats = 
  read.csv("./result/question-4/priority-task-life-cycle.csv", header = TRUE)

colnames(priorityLifeCycleStats)[colnames(priorityLifeCycleStats) == 'evicted_task_count'] <- 'evicted_count'
colnames(priorityLifeCycleStats)[colnames(priorityLifeCycleStats) == 'failed_task_count'] <- 'failed_count'
colnames(priorityLifeCycleStats)[colnames(priorityLifeCycleStats) == 'killed_task_count'] <- 'killed_count'
colnames(priorityLifeCycleStats)[colnames(priorityLifeCycleStats) == 'lost_task_count'] <- 'lost_count'
colnames(priorityLifeCycleStats)[colnames(priorityLifeCycleStats) == 'distinct_task_count'] <- 'task_count'

priorityLifeCycleStats$evicted_percent = round( priorityLifeCycleStats$evicted_count * 100 / priorityLifeCycleStats$task_count, 1 )
priorityLifeCycleStats$failed_percent =  round( priorityLifeCycleStats$failed_count * 100 / priorityLifeCycleStats$task_count, 1 )
priorityLifeCycleStats$killed_percent =  round( priorityLifeCycleStats$killed_count * 100 / priorityLifeCycleStats$task_count, 1 )
priorityLifeCycleStats$lost_percent =  round( priorityLifeCycleStats$lost_count * 100 / priorityLifeCycleStats$task_count, 1 )

#kable( formatDataFrame( priorityLifeCycleStats[,c("priority", "evicted_count", "failed_count", "killed_count", "lost_count", "task_count")]))
kable( formatDataFrame( priorityLifeCycleStats[,c("priority","killed_percent", "evicted_percent", "failed_percent", "lost_percent", "task_count")]),
       caption = "Task event percentages according to priority of tasks")


```

\pagebreak

##5. The relation between the Priority of a Task and Machine Resources(CPU, Memory)

Is there any relation between the priority of tasks and machine resources which tasks are scheduled? I have used task events and machine events tables to answer this question. I have calculated distinct machine table from machine events and I have joined it with task events table than I have grouped by priority and *capacity* then I have count the number of tasks scheduled.

Graph 2 shows a heat map. In this heat map,  on the y-axis, we have the priorities of the tasks and on the x-axis, we have the capacity of the machines which are tasked scheduled. On the cell's we have the percentages of tasks which are scheduled on that kind of machines. Table 7 shows the number of tasks according to the priority of a task. Also, Table 1 shows the percentages of the machines according to capacity. **When we consider all the information on the heat map and tables we can conclude that tasks are uniformly distributed on the machines according to priority. There is no relation between the priority of task and capacity of the machines.**

For example, In the cluster, 53.5% of the machines have the capacity of 0.49975(from Table 1). When we look at the values of the cell's on the heat map with capacity 0.49975 we can see that approximately 55% of the tasks on all categories are scheduled on this kind of machines. We can do the same observation for other capacities. 

&nbsp;
&nbsp;
&nbsp;

```{r echo=FALSE, fig.height = 4}
library(ggplot2)
library(ggExtra)
library(knitr)


priorityCapacityDataset = 
  read.csv("./result/question-5/priority-machine-capacity-relation.csv", header = TRUE)


# p=ggplot(priorityCapacityDataset, aes(x=capacity, y=priority, size=task_count, label=task_count)) +
#       geom_point() + 
#       #geom_text(aes(label=task_count),hjust=0, size=4, nudge_x= 0.025) +
#       theme(legend.position="none", axis.text.x = element_text(angle = 50, hjust = 1)) 
# 
# p +  scale_x_continuous(breaks=priorityCapacityDataset$capacity) + 
#   scale_y_continuous(breaks=unique(priorityCapacityDataset$priority))

taskCountByPriority <- aggregate(priorityCapacityDataset$task_count, by=list(Category=priorityCapacityDataset$priority), FUN=sum)

colnames(taskCountByPriority) <- c("priority","total_task_count")

#kable( formatDataFrame( taskCountByPriority ), caption="Task count by priority")

machineDistribution = 
  read.csv("./result/question-1/machine-dist-cpu-memory-capacity.csv", header = TRUE)
#kable(machineDistribution[,c("capacity","cpu", "memory", "machine_percentage")], caption="Machine distribution by capacity")


priorityCapacityDataset <- merge(priorityCapacityDataset, taskCountByPriority, by = "priority")

priorityCapacityDataset$task_percentage = round( priorityCapacityDataset$task_count * 100 /  priorityCapacityDataset$total_task_count, 1 ) 

#Makes disceret axis
priorityCapacityDataset$capacity <- as.character(priorityCapacityDataset$capacity)
priorityCapacityDataset$priority <- as.character(priorityCapacityDataset$priority)

#Order Values numerically
priorityCapacityDataset$priority <- factor(priorityCapacityDataset$priority, levels = c("0","1","2","3","4","5","6","7","8","9","10","11"))

ggplot(data = priorityCapacityDataset, aes(x = capacity, y = priority)) +
  geom_tile(aes(fill = task_percentage), colour = "white") +
  geom_text(aes(label=task_percentage),size=2,color="linen") +
  theme(legend.position="none",axis.text.x = element_text(angle = 50, hjust = 1), panel.grid.major  = element_blank(),  panel.grid.minor  = element_blank()) +
  ggtitle("Graph 2: Machine capacity and priority heat map") +
  theme(plot.title = element_text(hjust = 0.5, size = 9))
```

\pagebreak

Table 7 shows the task distribution according to priority. In this table, task counts are not unique task counts because some tasks are rescheduled because of eviction or failure.

```{r, echo=FALSE}
kable( formatDataFrame( taskCountByPriority ), caption = 'Task counts according to priority of the task')
#kable(machineDistribution, caption="Machine distribution according to capacity")
```

\pagebreak

##6. Tasks consume significantly less resource than what they requested

I have used a special method to decide whether or not a task is using significantly less resource from what it requested. To decide this, I have looked at the tasks' maximum CPU and memory resource consumption. If both of the maximum values never reaches what it requested. I assumed that this task is using significantly less resource from what it requested. I have used **task events** and **task usage** tables to find the specific tasks. I have joined this two table then I have grouped by according to job_id and task_index and I have looked at the max values of maximum CPU usage and maximum memory usage. Then I have decided whether or not a task using significantly less resource.

I have decided to create a histogram to have a better understanding of the resource consumption behavior of tasks which are using significantly less resources from what they requested. I have created two histograms. On the **X** axis I showed the resource consumption percentage according to what it requested on the **Y** I have shown the number of tasks. Graph 3 shows the CPU consumption behavior and Graph 4 shows the memory consumption behavior of the tasks which are using significantly fewer resources from what they requested.


```{r results='asis', echo=FALSE, fig.height = 2.5}

bins <- c(0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100)

memoryConsumption <- c(9403495, 162321, 212388, 488168, 528610, 594301, 829299, 598128, 485174, 514702, 602390, 509211, 521277, 507115, 512200, 559509, 624598, 661028, 796245, 1051587)

cpuComnsumprion <- c(11828731, 3196895, 1439782, 1691237, 685753, 321222, 225548, 168018, 123777, 106177, 69096, 142918, 36677, 17153, 17901, 19113, 15198, 15081, 22789, 18680)

consumptionDF <- data.frame(
  "percentage"= c(2.5, 7.5, 12.5, 17.5, 22.5, 27.5, 32.5, 37.5, 42.5, 47.5, 52.5, 57.5, 62.5, 67.5, 72.5, 77.5, 82.5, 87.5, 92.5, 97.5),
  "memory_consumption" = c(9403495, 162321, 212388, 488168, 528610, 594301, 829299, 598128, 485174, 514702, 602390, 509211, 521277, 507115, 512200, 559509, 624598, 661028, 796245, 1051587),
  "cpu_consumption" = c(11828731, 3196895, 1439782, 1691237, 685753, 321222, 225548, 168018, 123777, 106177, 69096, 142918, 36677, 17153, 17901, 19113, 15198, 15081, 22789, 18680)
)

p1 <- ggplot(consumptionDF, aes(x=percentage, y=memory_consumption)) +
  geom_bar(stat="identity") + scale_x_continuous(breaks=c(0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100)) +
  theme(legend.position="none", axis.text.x = element_text(angle = 50, hjust = 1, size=6)) + scale_y_log10() + labs(y = "Number of Tasks", x="resource consumption percentage") +
  ggtitle("Graph 3: Tasks consumes significantly less CPU") +
  theme(plot.title = element_text(hjust = 0.5, size = 8))

p2 <- ggplot(consumptionDF, aes(x=percentage, y=cpu_consumption)) +
  geom_bar(stat="identity") + scale_x_continuous(breaks=c(0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100)) +
  theme(legend.position="none", axis.text.x = element_text(angle = 50, hjust = 1, size=6)) + scale_y_log10() + labs(y = "Number of Tasks", x="resource consumption percentage") +
  ggtitle("Graph 4: Tasks consumes significantly less memory") +
  theme(plot.title = element_text(hjust = 0.5, size = 8))

grid.arrange(p1, p2, nrow = 1)

```


From these two histograms, we can conclude that there is around 10 million scheduled task which is using less than 5 percent of CPU and memory from what they requested.


##7. Machine availability

I have conducted some analysis to have an idea related to machine availability. 


*1- What is the average time between two consecutive machines remove event.*

>Average time between two consecutive remove event is *279.7* Seconds.
>
>According to the documentation, the machine remove events can occur because of failures or maintenance
>so we can conclude that every *279.7* second there is a failure or maintenance in the cluster.

*2- Are there any machine which is removed from the cluster and did not add to the back.*

> *97* of the machines are removed from the cluster and they are not added back.

\pagebreak

##8. Machines which resources are largely underused 

I have used task events table and task usage table to find machines which are resources largely underused. 

When we check according to scheduled task counts: 
In the cluster, there is 1 machine which has not any task scheduled.
In the cluster, there are 45 machines which have less than 10 tasks scheduled. There are 238 machines which have less than 100 tasks scheduled.

When we check according finished task counts: 
In the cluster, there are 52 machines which have not any task finished.
In the cluster, there are 98 machines which have less than 10 tasks finished. There are 336 machines which have less than 100 tasks finished.

When we try to see the underused machines from task usage table we have a different story.
When we grouped all the task usages according to the machine there are 36 machines which have not used.
I have tried to measure a different metric which is machine utilization metric. For every task usage, I have calculated utilization as follows

> machine_utilization = ( (task_end_time - task_start_time)/1000000 ) * assigned_memory_usage * sampled_CPU_usage)

Then, I have grouped by task machine utilization parameters according to machine id and I have summed the machine utilization metric.
According to this calculation, there are 36 machines which never utilized.

Following two histogram shows the task schedule counts per machine and task finished counts per machine. There is a machine which has not a task scheduled on and there are some machines which have not finished a task.


```{r, echo=FALSE, fig.height = 2.2}

options(warn=-1)

machineUtilization_scheduled_tasks = 
  read.csv("./result/question-6/scheduled-task-counts-on-machines.csv", header = TRUE)

p1<-ggplot(machineUtilization_scheduled_tasks, aes(x=scheduled_task_count)) + 
  geom_histogram(color="black", fill="white", binwidth=50)

p1 <- p1 + ggtitle("Graph 5: Scheduled task count per machine histogram") + theme(plot.title = element_text(hjust = 0.5, size = 8)) + labs(x = "scheduled task count", y = "number of machines")

p1

```

&nbsp;

```{r, echo=FALSE, fig.height = 2.2}

machineUtilization_finished_tasks = 
  read.csv("./result/question-6/finished-task-counts-on-machines.csv", header = TRUE)


p2<-ggplot(machineUtilization_finished_tasks, aes(x=scheduled_task_count)) + 
  geom_histogram(color="black", fill="white", binwidth=50)

p2 <- p2 + ggtitle("Graph 6: Finished task count per machine histogram") + theme(plot.title = element_text(hjust = 0.5, size = 8)) + labs(x = "finished task count", y = "number of machines")

p2

```





\pagebreak


Table 8 shows the statistics related to the scheduled task per machine and finished task per machines.

```{r, echo=FALSE}

df <- data.frame("scheduled_task_stats"=machineUtilization_scheduled_tasks$scheduled_task_count, "finished_task_stats"=machineUtilization_finished_tasks$scheduled_task_count)
summaryDF2 <- data.frame(unclass(apply(df, 2, summary)), check.names = FALSE, stringsAsFactors = FALSE)
colnames(summaryDF2)<- c("scheduled_task_stats", "finished_task_stats")

kable( summaryDF2, caption = "Machine scheduled and finished task statistics per machine")

```





#Performance Analyses of Apache Spark

##1. Effect of Number of Worker Thread

If there is enough input file to process than in this case the number of worker threads affecting performance. If there is only one input file to process than the number of worker threads has a small performance effect. This Effect is not linear. For example, to count all the distinct jobs in job tables with 8 worker thread takes 43 seconds. The same process takes 120 seconds with only one thread. For only one input file with a single worker thread the same job takes 15 seconds and with 8 worker thread, it takes 14 seconds.


##2. Effect of JVM Memory Size

Normally in the spark cluster, there is two kind of process driver and executor. There is two different parameter to control the JVM memory size. These parameters are "spark.driver.memory" and "spark.executor.memory". Normally this two-parameter have a different effect but in our case "spark.executor.memory" parameter is not useful because we are running Spark in the local mode which means that driver process spawning executor threads to do the processing so there is only one JVM which is driver JVM. Because of this reason, I can only assess the effect of "spark.driver.memory" parameter.

When we increase the memory size of the driver process, we can easily observe via using system tools(system monitor, top, htop) driver JVM is instantiated with specified memory and it is using the reserved memory. Also for caching, spark is using memory on default so if there is enough memory caching effect could be big in case of all the cached data small enough to fit in memory.

During my study, I have java.lang.OutOfMemoryError in case of collecting a huge amount of data at driver process. Arranging the JVM memory size could solve this problem.

##3. Effect of Lazy Evaluation

Spark has two important notion: transformation and action. If there is no action than spark does not apply the transformations. This is called lazy evaluation. If there is action than spark applies transformations to RDD's and it produces results. This means that if you do not have any action in your spark code than spark will not do anything and it will return very fast. Lazy evaluation gives the opportunity to spark optimize user queries in terms of processed data. There is no way to force Spark to evaluate transformations eagerly.

##4. Effect of Cache (Persisting)

Cache has a big impact on performance. In normal conditions spark is not caching RDD's. Assume that you are making a calculation and you are counting the number of elements in the RDD later you tried to save the RDD to a CSV file. Without caching Spark will calculate same RDD two times, once for counting once for writing to CSV file. With caching it will only calculate once and it will use the result of the same calculation for counting and writing to CSV file.

At the beginning of this study I was showing the result of the RDD's and later I was trying to do further processing without caching. In this case, it was taking two times more time to finish the job. Later, I realized the effect of caching and I tried to use caching when it is necessary.

#Conclution

In this worked, I have used Spark to analyze the trace of the Google cluster. I have used the Data Frame API. I have an opportunity to see the workload of a real cluster. I have evaluated the cluster load and machines of the cluster. I have evaluated the performance of the Spark with big data.


